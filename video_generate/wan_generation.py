"""
This script demonstrates how to generate a video using the CogVideoX model with the Hugging Face `diffusers` pipeline.
The script supports different types of video generation, including text-to-video (t2v), image-to-video (i2v),
and video-to-video (v2v), depending on the input data and different weight.

- text-to-video: THUDM/CogVideoX-5b, THUDM/CogVideoX-2b or THUDM/CogVideoX1.5-5b
- video-to-video: THUDM/CogVideoX-5b, THUDM/CogVideoX-2b or THUDM/CogVideoX1.5-5b
- image-to-video: THUDM/CogVideoX-5b-I2V or THUDM/CogVideoX1.5-5b-I2V

Running the Script:
To run the script, use the following command with appropriate arguments:

```bash
$ python cli_demo.py --prompt "A girl riding a bike." --model_path THUDM/CogVideoX1.5-5b --generate_type "t2v"
```

Additional options are available to specify the model path, guidance scale, number of inference steps, video generation type, and output paths.
"""

import argparse
from typing import Literal

import torch
# from diffusers import (
#     CogVideoXImageToVideoPipeline,
#     CogVideoXVideoToVideoPipeline,
# )
from scheduler_wan import FlowMatchEulerDiscreteScheduler
from pipeline_wan import WanImageToVideoPipeline

from diffusers.utils import export_to_video, load_image, load_video
import json 
import re 
import random 
import numpy as np 
import cv2 
from einops import rearrange
import os 
from tqdm import tqdm 
import torchvision.transforms.v2 as tr
import imageio
from torchvision.transforms import InterpolationMode
from PIL import Image 
from diffusers import AutoencoderKLWan
from diffusers.utils import export_to_video, load_image
from transformers import CLIPVisionModel


def set_random_seed(seed):
    """Set random seed for reproducability."""
    if seed is not None:
        global _GLOBAL_RANDOM_SEED
        _GLOBAL_RANDOM_SEED = seed
        assert seed > 0
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.enabled = True # False
        torch.backends.cuda.matmul.allow_tf32 = True  # allow TF32 for speed
        try:
            import deepspeed
            if deepspeed.checkpointing.is_configured():
                mpu.model_parallel_cuda_manual_seed(seed)
        except ImportError:
            pass


def sigma_matrix2(sig_x, sig_y, theta):
    """Calculate the rotated sigma matrix (two dimensional matrix).
    Args:
        sig_x (float):
        sig_y (float):
        theta (float): Radian measurement.
    Returns:
        ndarray: Rotated sigma matrix.
    """
    d_matrix = np.array([[sig_x**2, 0], [0, sig_y**2]])
    u_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
    return np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))


def mesh_grid(kernel_size):
    """Generate the mesh grid, centering at zero.
    Args:
        kernel_size (int):
    Returns:
        xy (ndarray): with the shape (kernel_size, kernel_size, 2)
        xx (ndarray): with the shape (kernel_size, kernel_size)
        yy (ndarray): with the shape (kernel_size, kernel_size)
    """
    ax = np.arange(-kernel_size // 2 + 1.0, kernel_size // 2 + 1.0)
    xx, yy = np.meshgrid(ax, ax)
    xy = np.hstack(
        (
            xx.reshape((kernel_size * kernel_size, 1)),
            yy.reshape(kernel_size * kernel_size, 1),
        )
    ).reshape(kernel_size, kernel_size, 2)
    return xy, xx, yy


def pdf2(sigma_matrix, grid):
    """Calculate PDF of the bivariate Gaussian distribution.
    Args:
        sigma_matrix (ndarray): with the shape (2, 2)
        grid (ndarray): generated by :func:`mesh_grid`,
            with the shape (K, K, 2), K is the kernel size.
    Returns:
        kernel (ndarrray): un-normalized kernel.
    """
    inverse_sigma = np.linalg.inv(sigma_matrix)
    kernel = np.exp(-0.5 * np.sum(np.dot(grid, inverse_sigma) * grid, 2))
    return kernel


def bivariate_Gaussian(kernel_size, sig_x, sig_y, theta, grid=None, isotropic=True):
    """Generate a bivariate isotropic or anisotropic Gaussian kernel.
    In the isotropic mode, only `sig_x` is used. `sig_y` and `theta` is ignored.
    Args:
        kernel_size (int):
        sig_x (float):
        sig_y (float):
        theta (float): Radian measurement.
        grid (ndarray, optional): generated by :func:`mesh_grid`,
            with the shape (K, K, 2), K is the kernel size. Default: None
        isotropic (bool):
    Returns:
        kernel (ndarray): normalized kernel.
    """
    if grid is None:
        grid, _, _ = mesh_grid(kernel_size)
    if isotropic:
        sigma_matrix = np.array([[sig_x**2, 0], [0, sig_x**2]])
    else:
        sigma_matrix = sigma_matrix2(sig_x, sig_y, theta)
    kernel = pdf2(sigma_matrix, grid)
    kernel = kernel / np.sum(kernel)
    return kernel


def read_points(files, video_len=16, reverse=False):
    output_points = []
    files = files.split(",")
    for file in files:
        with open(file, "r") as f:
            lines = f.readlines()
        points = []
        for line in lines:
            x, y = line.strip().split(",")
            points.append((int(x), int(y)))
        if reverse:
            points = points[::-1]

        if len(points) > video_len:
            skip = len(points) // video_len
            points = points[::skip]
        points = points[:video_len]
        output_points.append(points)

    return output_points


size = 99
sigma = 10
blur_kernel = bivariate_Gaussian(size, sigma, sigma, 0, grid=None, isotropic=True)
blur_kernel = blur_kernel / blur_kernel[size // 2, size // 2]


def parse_frames(input_text):
    frames = []
    # Improved pattern for parsing frames and captions
    pattern = re.compile(
        r'Frame_(\d+):\s*(\[\[.*?\]\]|\[.*?\])\s*,\s*caption:\s*(.*?)(?=(?:Frame_\d+:|\Z))',
        re.DOTALL
    )
    matches = pattern.finditer(input_text)
    for match in matches:
        frame_number = int(match.group(1))
        objects_list_str = match.group(2).strip()
        caption = match.group(3).strip()
        
        if "Reasoning:" in caption:
            caption = caption.split("Reasoning:")[0].strip("*").strip("\n")

        # Parse the objects list string into a Python list
        try:
            objects_list = json.loads(f"{objects_list_str}")
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON in frame {frame_number}: {e}")
            continue

        frames.append({
            'frame_number': frame_number,
            'objects': objects_list,
            'caption': caption
        })
    return frames


def process_traj_data(points_files, num_frames, video_size, device="cpu"):
    optical_flow = np.zeros((num_frames, video_size[0], video_size[1], 2), dtype=np.float32)
    processed_points = []
    xy_range = 256
    h, w = video_size
    for points in points_files:
        points = process_points(points, num_frames)
        points = [[int(w * x / xy_range), int(h * y / xy_range)] for x, y in points]
        optical_flow = get_flow(points, optical_flow, video_len=num_frames)
        processed_points.append(points)

    for i in range(1, num_frames):
        optical_flow[i] = cv2.filter2D(optical_flow[i], -1, blur_kernel)

    optical_flow = torch.tensor(optical_flow).to(device)

    return optical_flow, processed_points


def process_points(points, frames):
    defualt_points = [[512, 512]] * frames

    if len(points) < 2:
        return defualt_points
    elif len(points) >= frames:
        skip = len(points) // frames
        return points[::skip][: frames - 1] + points[-1:]
    else:
        insert_num = frames - len(points)
        insert_num_dict = {}
        interval = len(points) - 1
        n = insert_num // interval
        m = insert_num % interval
        for i in range(interval):
            insert_num_dict[i] = n
        for i in range(m):
            insert_num_dict[i] += 1

        res = []
        for i in range(interval):
            insert_points = []
            x0, y0 = points[i]
            x1, y1 = points[i + 1]

            delta_x = x1 - x0
            delta_y = y1 - y0
            for j in range(insert_num_dict[i]):
                x = x0 + (j + 1) / (insert_num_dict[i] + 1) * delta_x
                y = y0 + (j + 1) / (insert_num_dict[i] + 1) * delta_y
                insert_points.append([int(x), int(y)])

            res += points[i : i + 1] + insert_points
        res += points[-1:]
        return res


def get_flow(points, optical_flow, video_len):
    for i in range(video_len - 1):
        p = points[i]
        p1 = points[i + 1]
        if p[0] < 0:
            continue
        elif p[0] > 0 and p1[0] < 0:
            continue
        optical_flow[i + 1, p[1]-1, p[0]-1, 0] = p1[0] - p[0]
        optical_flow[i + 1, p[1]-1, p[0]-1, 1] = p1[1] - p[1]

    return optical_flow


def generate_video(
    prompt: str,
    model_path: str,
    lora_path: str = None,
    lora_rank: int = 128,
    num_frames: int = 81,
    width: int = 1360,
    height: int = 768,
    output_path: str = "./output.mp4",
    first_frame_path: str = "",
    num_inference_steps: int = 50,
    guidance_scale: float = 6.0,
    num_videos_per_prompt: int = 1,
    dtype: torch.dtype = torch.bfloat16,
    generate_type: str = Literal["t2v", "i2v", "v2v"],  # i2v: image to video, v2v: video to video
    seed: int = 42,
    fps: int = 8,
    latent_path: str = "",
    skill_name: str = "4_motion_binding",
    prompt_path: str = "",
    noise_steps: str = "699",
    cache_dir: str = "",
):
    """Generate video and save to disk."""

    set_random_seed(seed)
    image = None
    video = None

    CACHE_DIR = cache_dir

    # Sharded, bf16 model loading across all visible GPUs
    image_encoder = CLIPVisionModel.from_pretrained(
        model_path, 
        subfolder="image_encoder", 
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        device_map="balanced",
    )

    vae = AutoencoderKLWan.from_pretrained(
        model_path, 
        subfolder="vae", 
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        # device_map="balanced",
    )

    pipe = WanImageToVideoPipeline.from_pretrained(
        model_path, 
        vae=vae, 
        image_encoder=image_encoder, 
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        device_map="balanced",
    )

    # If you're using LoRA, add this code
    if lora_path:
        pipe.load_lora_weights(
            lora_path,
            weight_name="pytorch_lora_weights.safetensors",
            adapter_name="test_1",
            cache_dir=CACHE_DIR,
        )
        pipe.fuse_lora(lora_scale=1 / lora_rank)

    # Set Scheduler.
    pipe.scheduler = FlowMatchEulerDiscreteScheduler.from_config(
        pipe.scheduler.config, timestep_spacing="trailing"
    )

    pipe.vae.requires_grad_(False)

    with open(os.path.join(prompt_path, f"{skill_name}.txt"), "r") as file:
        lines = file.readlines()

    eval_prompts = [line.strip() for line in lines]

    os.makedirs(output_path, exist_ok=True)
    os.makedirs(f"{output_path}/{skill_name}", exist_ok=True)

    for i, prompt in enumerate(tqdm(eval_prompts)):
        index = i + 1
        images = []
        latent_dir = f"{latent_path}/{skill_name}/{index:04d}/frames"
        if not os.path.exists(latent_dir):
            print(f"Latent path for skill {skill_name} index {index} not found, skipping.")
            continue

        first_frame = (
            Image.open(f"{first_frame_path}/{skill_name}/{index:04d}.jpg")
            .convert('RGB')
            .resize((832, 480))
        )
        images.append(first_frame)
        for j in range(1, 81):
            img_path = f"{latent_dir}/frame_{j:04d}.png"
            image = Image.open(img_path)
            images.append(image)

        frame_images = pipe.video_processor.preprocess(
            images, height=480, width=832
        )
        frame_images = rearrange(frame_images, "f c h w -> 1 c f h w")
        frame_images = frame_images.to(device=pipe.vae.device, dtype=pipe.vae.dtype)
        frame_images = frame_images.to(device=pipe.vae.device, dtype=pipe.vae.dtype)

        print(frame_images.shape, frame_images.dtype, frame_images.device)
        print(pipe.vae.device, pipe.vae.dtype)
        latent = pipe.vae.encode(frame_images).latent_dist.sample()
        latents_mean = (
            torch.tensor(pipe.vae.config.latents_mean)
            .view(1, pipe.vae.config.z_dim, 1, 1, 1)
            .to(latent.device, latent.dtype)
        )
        latents_std = 1.0 / torch.tensor(pipe.vae.config.latents_std).view(
            1, pipe.vae.config.z_dim, 1, 1, 1
        ).to(latent.device, latent.dtype)
        print(
            f"latent properties: {latent.shape}, {latent.dtype}, {latent.device}",
            f"latent_mean: {latents_mean.shape}, {latents_mean.dtype}, {latents_mean.device}",
            f"latent_std: {latents_std.shape}, {latents_std.dtype}, {latents_std.device}",
        )
        noise_images = (latent - latents_mean) * latents_std
        print(noise_images.shape)

        noise = torch.randn_like(noise_images)
        pipe.scheduler.set_timesteps(num_inference_steps)
        all_steps = list(pipe.scheduler.timesteps)
        print(all_steps)
        print(noise_steps)

        for noise_step in noise_steps.split(","):
            timestep = torch.tensor(int(noise_step), device=noise_images.device)
            sigma = pipe.scheduler.sigmas[timestep]
            print(f"noise_step: {noise_step}, sigma: {sigma}")
            noisy_video_latents = noise_images * (1.0 - sigma) + noise * (sigma)

            start_step = int(noise_step)
            timesteps = all_steps[start_step:]
            out_path = f"{output_path}/{skill_name}/{index:04d}_{noise_step}_{start_step}.mp4"
            print(noisy_video_latents.shape, noisy_video_latents.dtype, noisy_video_latents.device, pipe.vae.device, pipe.vae.dtype)            
            if os.path.exists(out_path):
                print(
                    f"Video for skill {skill_name} index {index} with noise steps {noise_steps} already exists, skipping."
                )
                continue
            prefix = noisy_video_latents
            video_generate = pipe(
                image=first_frame,
                prompt=prompt,
                num_videos_per_prompt=num_videos_per_prompt,
                num_inference_steps=len(timesteps),
                timesteps=timesteps,
                num_frames=num_frames,
                guidance_scale=guidance_scale,
                generator=torch.Generator().manual_seed(seed),
                latents=prefix,
            ).frames[0]

            export_to_video(video_generate, out_path, fps=fps)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a video from a text prompt using CogVideoX")
    parser.add_argument("--prompt", type=str, default=None, help="The description of the video to be generated")
    parser.add_argument(
        "--first_frame_path",
        type=str,
        default=None,
        help="The path of the first frame to be used as the image input of the video",
    )
    parser.add_argument(
        "--model_path", type=str, default="Wan-AI/Wan2.1-I2V-14B-480P-Diffusers", help="Path of the pre-trained model use"
    )
    parser.add_argument("--lora_path", type=str, default=None, help="The path of the LoRA weights to be used")
    parser.add_argument("--lora_rank", type=int, default=128, help="The rank of the LoRA weights")
    parser.add_argument("--output_path", type=str, default="./output.mp4", help="The path save generated video")
    parser.add_argument("--guidance_scale", type=float, default=6.0, help="The scale for classifier-free guidance")
    parser.add_argument("--num_inference_steps", type=int, default=50, help="Inference steps")
    parser.add_argument("--num_frames", type=int, default=81, help="Number of steps for the inference process")
    parser.add_argument("--width", type=int, default=1360, help="Number of steps for the inference process")
    parser.add_argument("--height", type=int, default=768, help="Number of steps for the inference process")
    parser.add_argument("--fps", type=int, default=8, help="Number of steps for the inference process")
    parser.add_argument("--num_videos_per_prompt", type=int, default=1, help="Number of videos to generate per prompt")
    parser.add_argument("--generate_type", type=str, default="t2v", help="The type of video generation")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="The data type for computation")
    parser.add_argument("--seed", type=int, default=42, help="The seed for reproducibility")
    parser.add_argument("--latent_path", type=str, default="")
    parser.add_argument("--skill_name", type=str, default="4_motion_binding")
    parser.add_argument("--prompt_path", type=str, default="")
    parser.add_argument("--noise_steps", type=str, default="699")
    parser.add_argument("--cache_dir", type=str, default="")

    args = parser.parse_args()
    dtype = torch.float16 if args.dtype == "float16" else torch.bfloat16
    generate_video(
        prompt=args.prompt,
        model_path=args.model_path,
        lora_path=args.lora_path,
        lora_rank=args.lora_rank,
        output_path=args.output_path,
        num_frames=args.num_frames,
        width=args.width,
        height=args.height,
        first_frame_path=args.first_frame_path,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        num_videos_per_prompt=args.num_videos_per_prompt,
        dtype=dtype,
        generate_type=args.generate_type,
        seed=args.seed,
        fps=args.fps,
        latent_path=args.latent_path,
        skill_name=args.skill_name,
        prompt_path=args.prompt_path,
        noise_steps=args.noise_steps,
        cache_dir=args.cache_dir,
    )
